{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn nest-asyncio pyngrok python-multipart pymupdf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8dfhdIFORrKg",
        "outputId": "ebbda0b2-a3ad-416a-c920-39f499ee3a14"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.116.1)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.35.0)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.3.0)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.11/dist-packages (0.0.20)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.26.3)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.47.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.14.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.48.0,>=0.40.0->fastapi) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (1.3.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-24' coro=<Server.serve() done, defined at /usr/local/lib/python3.11/dist-packages/uvicorn/server.py:69> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/main.py\", line 580, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 67, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 360, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 70, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 331, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-docx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1w6pkhAdPvGr",
        "outputId": "3518786e-7091-458c-806d-9741d742c465"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "6ruMxd9VRep-"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI, File, UploadFile\n",
        "from fastapi.responses import PlainTextResponse\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "from docx import Document\n",
        "import io\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "summarizer = pipeline(\"summarization\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6meL3PA4Rx9S",
        "outputId": "022dc3a1-d429-469a-d007-ea9fb9bac3a8"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken \"Paste your ngrok authtoken here\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hheS1uN6xMPt",
        "outputId": "e700c969-749c-4b6d-ff6c-ec8787d92995"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_text_from_file(contents, filename):\n",
        "\n",
        "    extension = filename.lower().split('.')[-1]\n",
        "\n",
        "    if extension == 'pdf':\n",
        "        doc = fitz.open(stream=contents, filetype='pdf')\n",
        "        text = \"\"\n",
        "        for page in doc:\n",
        "            text += page.get_text()\n",
        "        return text\n",
        "\n",
        "    elif extension == 'txt':\n",
        "        return contents.decode('utf-8')  # assuming contents is bytes\n",
        "\n",
        "    elif extension == 'docx':\n",
        "        doc = Document(io.BytesIO(contents))\n",
        "        text = '\\n'.join([p.text for p in doc.paragraphs])\n",
        "        return text\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file type\")\n"
      ],
      "metadata": {
        "id": "I35caxRwM6sC"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # Load English model\n",
        "\n",
        "def chunk_by_sentences_spacy(text, max_chunk_size=1000):\n",
        "    doc = nlp(text)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for sent in doc.sents:\n",
        "        sentence = sent.text.strip()\n",
        "        # Check if adding this sentence exceeds max chunk size\n",
        "        if len(current_chunk) + len(sentence) + 1 <= max_chunk_size:\n",
        "            current_chunk += (\" \" + sentence) if current_chunk else sentence\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "    return chunks\n",
        "\n",
        "# Test\n",
        "text = \"Ali is learning NLP. He is working on summarization! This project is interesting? Yes, it really is.\"\n",
        "chunks = chunk_by_sentences_spacy(text, max_chunk_size=50)\n",
        "print(chunks)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bi-eGP8ggHvb",
        "outputId": "d8321926-333f-48e5-d5c1-b648ccea4c85"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Ali is learning NLP.', 'He is working on summarization!', 'This project is interesting? Yes, it really is.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Ali is learning NLP. He is working on summarization. This project is interesting!\"\n",
        "chunks = chunk_by_sentences(text, max_chunk_size=50)\n",
        "print(chunks)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PviPKkuviHMq",
        "outputId": "87e87b78-9e7d-41ab-ffad-3cb23674129b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Ali is learning NLP.', 'He is working on summarization.', 'This project is interesting!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap"
      ],
      "metadata": {
        "id": "46joGqErlpEL"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app = FastAPI()\n",
        "\n",
        "@app.post(\"/upload\", response_class=PlainTextResponse)\n",
        "async def upload_pdf(file: UploadFile = File(...)):\n",
        "    contents = await file.read()\n",
        "    text = extract_text_from_file(contents=contents, filename=file.filename)\n",
        "\n",
        "    print(\"text is read from pdf \")\n",
        "    chunks = chunk_by_sentences(text=text)\n",
        "    print(\"chunks are made from text\")\n",
        "    # Summarize each chunk\n",
        "    summaries = []\n",
        "    for chunk in chunks:\n",
        "        summary = summarizer(chunk, max_length=130, min_length=30, do_sample=False)[0]['summary_text']\n",
        "        summaries.append(summary)\n",
        "    print(\"summaries is generated\")\n",
        "    print(summaries)\n",
        "    return \"\\n\".join(summaries)\n"
      ],
      "metadata": {
        "id": "PGLWvFugy-0I"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"http://127.0.0.1:5500\", \"http://localhost:5000\"],  # add all your frontend origins here\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n"
      ],
      "metadata": {
        "id": "26mU5xmlfY1I"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup for running inside Jupyter/Colab\n",
        "nest_asyncio.apply()\n",
        "ngrok.kill()\n",
        "public_url = ngrok.connect(8000)\n",
        "print(\"Public URL:\", public_url)\n",
        "\n",
        "# Run API\n",
        "uvicorn.run(app, port=8000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sr1V-JohRlud",
        "outputId": "6d159158-f4e1-4cc0-8ec6-c0b00f0d3e1f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://5c1c7e8467c0.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [243]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text is read from pdf \n",
            "chunks are made from text\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 130, but your input_length is only 124. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=62)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "summaries is generated\n",
            "[\" Pakistan, officially the Islamic Republic of Pakistan, is the fifth-most populous country in South Asia . Islamabad is the nation's capital, while Karachi is its largest city and financial centre . Pakistan is the 33rd-largest country by area .\", \" Pakistan is the site of several ancient cultures, including the 8,500-year-old Neolithic site of Mehrgarh in Balochistan . It shares a maritime border with Oman in the Gulf of Oman and is separated from Tajikistan in the northwest by Afghanistan's narrow Wakhan Corridor . Pakistan gained independence in 1947 after the partition of the British Indian Empire .\", \" In 1971, the exclave of East Pakistan seceded as the new country of Bangladesh after a nine-month-long civil war . Pakistan's political history has been characterized by periods of significant economic and military growth as well as those of political and economic instability . The country continues to face challenges, including poverty, illiteracy, corruption, and terrorism .\", ' Pakistan is designated as a major non-NATO ally by the United States . The name Pakistan was coined by Choudhry Rahmat Ali, a Pakistan Movement activist . Pakistan is a member of the United Nations, the Shanghai Cooperation Organisation, the Organisation of Islamic Cooperation, the Commonwealth of Nations and the South Asian Association for Regional Cooperation .', \" The name means the land of the Paks, the spiritually pure and clean . Etymologists note that  is 'pure' in Persian and Pashto and the Persian suffix  means 'land'\"]\n",
            "INFO:     103.179.246.161:0 - \"POST /upload HTTP/1.1\" 200 OK\n",
            "text is read from pdf \n",
            "chunks are made from text\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 130, but your input_length is only 22. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "summaries is generated\n",
            "[' The Himalayas are home to Mount Everest, the tallest peak on Earth . The world is filled with an incredible variety of landscapes, cultures, and natural wonders . Exploring beautiful places around the globe offers a feast for the eyes and the soul .', ' The Rocky Mountains stretch from Canada to the United States . The Alps are renowned for their picturesque villages, skiing resorts, and lush valleys dotted with wildflowers during the summer .', ' The Maldives, an archipelago of over a thousand coral islands, is a paradise for beach lovers and honeymooners . The Rocky Mountain National Park in Colorado is a popular destination, showcasing dramatic cliffs, alpine lakes, and vast forests .', ' The Caribbean is famous for its vibrant culture, reggae music, and lively festivals . The Great Barrier Reef in Australia is the largest coral reef system in the world . The western coast of the United States offers dramatic cliffs and expansive beaches .', ' The city of Rome, Italy, is a living museum filled with ruins from the Roman Empire . Petra in Jordan is an archaeological marvel carved into red sandstone cliffs . Istanbul, Turkey, sits at the crossroads of Europe and Asia, blending Byzantine, Ottoman, and modern influences .', ' The Serengeti National Park in Tanzania is world-renowned for its vast savannahs and the Great Migration of wildebeest and zebras . The Amazon Rainforest, spanning multiple countries, is the largest tropical rainforest in the world .', ' Banff National Park in Canada is known for its turquoise lakes, snowy peaks, and abundant wildlife like elk and bears . Santorini, Greece, with its iconic whitewashed buildings and blue-domed churches overlooking the Aegean Sea, is a symbol of Mediterranean beauty .', ' Seychelles is an archipelago of 115 islands featuring granite boulders, palm-fringed beaches, and rare species of flora and fauna . Singapore is a shining example of a green city, with its futuristic skyline surrounded by lush gardens and clean streets . Dubai has transformed from a desert city to a glittering metropolis with the Burj Khalifa .', \" The contrast between desert dunes and skyscrapers is striking and shows human creativity at its peak . Traveling to beautiful places opens the door to new experiences and deeper understanding of the world's complexity and richness .\", \" Protecting these beautiful places is vital so future generations can also marvel at the wonders of our planet . Protecting them is vital for future generations to marvel at our planet's wonders .\"]\n",
            "INFO:     103.179.246.161:0 - \"POST /upload HTTP/1.1\" 200 OK\n",
            "text is read from pdf \n",
            "chunks are made from text\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 130, but your input_length is only 115. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=57)\n",
            "Your max_length is set to 130, but your input_length is only 95. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=47)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "summaries is generated\n",
            "[' Google hereby grants permission to produce the tables and figures in this paper solely for use in journalistic or scholarly works . We propose a new simple network architecture, the Transformer, based solely on attention mechanisms .', ' Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train . We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing with large and limited training data .', ' Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work . Niki designed, implemented, tuned and evaluated countless model variants in our original codebase . Llion also experimented with novel model variants, was responsible for our initial codebase, and was responsible . Noam proposed scaled dot-product attention, multi-headcentricattention and the . parameter-free position representation .', ' Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor . They replaced our earlier codebase, greatly improving results and massively accelerating research .', ' Sequential nature precludes parallelization within training examples . Memory constraints limit batching across examples . Attention mechanisms have become an integral part of compelling sequence modeling and transduc-tion models .', ' The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .', ' In these models, the number of operations required to relate signals from two arbitrary input or output positions grows exponentiallyin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet . This makes it more difficult to learn dependencies between distant positions [12] In the Transformer this is reduced to a constant number of . operations, albeit at the cost of reduced effective resolution .', ' Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization and learning task-independent sentence representations [4, 27, 28, 22]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence .', ' Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35] In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages .', ' The Transformer follows this overall architecture using stacked self-attention and point-wise, fully-connected layers for both the encoder and decoder . At each step the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next .', ' Decoder is composed of a stack of N = 6 identical layers . All sub-layers in the model, as well as the embedding, produce outputs of dimension dmodel = 512 .', ' An attention function can be described as mapping a query and a set of key-value pairs to an output . We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2) Multi-Head Attention consists of several layers running in parallel .', ' The two most commonly used attention functions are additive attention [2] and dot-product attention . Additive attention computes the compatibility function using a feed-forward network . Dot product attention is identical to our algorithm, except for the scaling factor .', ' Additive attention outperforms dot product attention without scaling for larger values of dk [3] We suspect that for large values, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients .', ' In this work we employ h = 8 parallel attention layers, or heads . Headi = Attention(QW Q, K, V, V ) = Attention . Headh = Concat(head1, ... ..., ..., headh)W O.', ' The Transformer uses multi-head attention in three different ways . In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer . This allows every position in the decoder to attend over all positions in the input sequence .', ' Each position in the encoder and decoder can attend to all positions in the previous layer of the encodeoder . Self-attention layers in the decoder allow each position to attend to other layers up to and including that position . We implement this with masking out (setting to −∞) all values in the inputmax which correspond to illegal connections .', ' We use learned embeddings to convert the input and output tokens to vectors of dimension dmodel . We also use the usual learned linear transfor-uctivemation and softmax function to convert decoder output to predicted next-token probabilities .', ' Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types . n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention .', ' The positional encodings have the same dimension dmodelas the embeddings, so that the two can be summed . Each dimension of the positional encoding corresponds to a sinusoid . The wavelengths form a geometric progression from 2π to 10000 · 2π .', ' We chose the sinusoidal version of self-attention because it may allow the model to extrapolate to sequence lengths longer than those encountered during training .', ' The shorter paths between any combination of positions in the input and output sequences, the shorter these paths, the easier it is to learn long-range dependencies [12]. Self-attention layers are faster than recurrent layers when the sequence n is smaller than the representation dimensionality d .', ' To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position . This would increase the maximum path length to O(n/r)', ' The complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer . We inspect attention distributions from our models and present examples in the appendix .', ' We used the significantly larger WMT2014 English-French dataset consisting of 36M sentences . Sentence pairs were batched together by approximate sequence length . Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens .', ' We varied the learning rate over the course of training, according to the formula:\\xa0reproportionally\\xa0to the inverse square root of the step number . This corresponds to increasing learning rate linearly for the first warmup_steps training steps .', ' The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and French newstest tests .', ' The big transformer model outperforms the best previously reported models (including ensembles) by more than 2.0% . Training took 3.5 days on 8 P100 GPUs .', ' The Transformer (big) model trained for English-to-French used a beam search with a beam size of 4 and length penalty α = 0.6 [38]. We used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals .', ' Table 2 summarizes our results and compares our translation quality and training costs to other model-architectures from the literature . We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100 .', ' We used beam search as described in the previous section, but no point-avering . We vary the number of attention heads and the amount of computation constant . We present these results in Table 3.2 .', ' In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality . While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads . This suggests a more sophisticated compatibility function may be beneficial .', ' We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences . We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]', ' The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ) During inference, we provide the maximum output length to input length + 300 .', ' We used a beam size of 21 and α = 0.3 for both WSJ only and the semi-supervised setting . The Transformer outperforms the Berkeley-glyph Parser, even when training only on the WSJ training set of 40K sentences .', ' We are excited about the future of attention-based models and plan to apply them to other tasks . We plan to extend the Transformer to problems involving input and output modalities other than text and video .', ' Aims to explore neural-network grammars for machine translation . The results are published in the form of arXiv.com/ArXiv: 2017 .', ' Researchers have used deep residual learning for im-ensiblyage recognition . They have also used recurrent neural networks to generate sequences with recurrent networks . The results are published in the journal ArXiv: 1705.03122v2, 2017 .', ' The limits of language modeling are explored by Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu .', ' Researchers: A structured self-attentive sentence embedding . The results are published in the Human Language Technology Conference of the NAACL Main Conference .', ' Researchers: A decomposable attention-driven attention model for abstractive language summaries . They: A deep reinforced model for . abstractive.summarization. An algorithm that learns accurate, compact, . and interpretable tree annotation. An approach to machine translation of rare words.', ' The sparsely-gated mixture-of-expertslayer layer is a way to avoid overfitting neural networks . The study was published in the journal of Machine-Machine Learning Research .', ' Google’s neural machine. machine-translation system: Bridging the gap between human and machine translation. In. the ACL (Volume.1: Long Papers), pages 434–443.', ' An example of the attention mechanism following long-distance dependencies in the phrase ‘making...more difficult’ Different colors represent different heads .', ' Figure 4: Two attention heads, also in layer 5 of 6, were apparently involved in anaphora resolution . Figure 5: Isolated attentions from just the word ‘its’ for attention heads 5 and 6 .', ' Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence structure . The heads clearly learned to perform different tasks using the encoder self-attention .']\n",
            "INFO:     103.179.246.161:0 - \"POST /upload HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [243]\n"
          ]
        }
      ]
    }
  ]
}